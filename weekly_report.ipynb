{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "import mysql.connector\n",
    "from mysql.connector import errorcode\n",
    "from mysql.connector.pooling import MySQLConnectionPool\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from level1_cleaning import level1_cleaning_site\n",
    "\n",
    "mpl.rc(\"font\", family=\"Helvetica\")\n",
    "mpl.rc(\"font\", size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def init():\n",
    "    global cnx\n",
    "    cnx = mysql.connector.connect(user=\"root\", password=\"root\", database=\"ar_data\")\n",
    "    site_info_query_command = \"SELECT * FROM sites\"\n",
    "    site_table = pd.read_sql_query(site_info_query_command, cnx)\n",
    "    return site_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_pool():\n",
    "    global pool\n",
    "    dbconfig = {\n",
    "        \"database\": \"ar_data\",\n",
    "        \"user\": \"root\",\n",
    "        \"password\": \"root\"\n",
    "    }\n",
    "    pool = MySQLConnectionPool(pool_name = \"para_pool\", pool_size = 10, **dbconfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define week start datetime, this needs to be automated after this report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def node_info_query_by_site(site_table, start_time):\n",
    "    valid_motes = np.empty((0, 4))\n",
    "    for index, site_info in site_table.iterrows():\n",
    "        site_id = site_info['site_id']\n",
    "        site_name = site_info['site_name']\n",
    "        node_info_query_command = \"SELECT node_id, elevation, server_last_update, \" + \\\n",
    "                                  \" sd_last_update, ground_dist FROM motes WHERE site_id = \" + str(site_id)\n",
    "        node_table = pd.read_sql_query(node_info_query_command, cnx)\n",
    "        for node_index, node_info in node_table.iterrows():\n",
    "            node_id = node_info['node_id']\n",
    "            elevation = node_info['elevation']\n",
    "            server_last_update = node_info['server_last_update']\n",
    "            sd_last_update = node_info['sd_last_update']\n",
    "            ground_dist = node_info['ground_dist']\n",
    "            if ground_dist is None:\n",
    "                continue\n",
    "            if server_last_update is None and sd_last_update is None:\n",
    "                continue\n",
    "            if np.isnan(ground_dist):\n",
    "                continue\n",
    "            if server_last_update is None and sd_last_update < start_time:\n",
    "                continue\n",
    "            if sd_last_update is None and server_last_update < start_time:\n",
    "                continue\n",
    "            if sd_last_update is not None and server_last_update is not None:\n",
    "                if sd_last_update < start_time and server_last_update < start_time:\n",
    "                    continue\n",
    "            valid_motes = np.vstack((valid_motes, np.array([site_id, node_id, elevation, ground_dist])))\n",
    "    return valid_motes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_snowdepth_para(site_id, days=1, start_time=datetime.now()):\n",
    "    print \"Start cleaning snowdepth from site\", site_id\n",
    "    clean_cnx = pool.get_connection()\n",
    "    for dt in range(0, days):\n",
    "        temp_datetime = start_time + timedelta(days=dt)\n",
    "        level1_cleaning_site(site_id, temp_datetime, temp_datetime + timedelta(days=1), clean_cnx)\n",
    "    clean_cnx.close()\n",
    "    print \"Finished cleaning snowdepth from site\", site_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_snowdepth(site_table, start_time, ending_time = None):            \n",
    "    # For weekly snowdepth cleaning\n",
    "    if ending_time is None:\n",
    "        for index, site_info in site_table.iterrows():\n",
    "            site_id = site_info['site_id']\n",
    "            for dt in range(0, 7):\n",
    "                temp_datetime = start_time + timedelta(days=dt)\n",
    "                print site_id, temp_datetime, \"start to clean\"\n",
    "                level1_cleaning_site(site_id, temp_datetime, temp_datetime + timedelta(days=1))\n",
    "        return\n",
    "    # For long term snowdepth cleaning\n",
    "    else:\n",
    "        days = (ending_time - start_time).days\n",
    "        all_sites_id = []\n",
    "        for index, site_info in site_table.iterrows():\n",
    "            site_id = site_info['site_id']\n",
    "            all_sites_id.append(site_id)\n",
    "        clean_snowdepth_para_partial = partial(clean_snowdepth_para, \n",
    "                                               days=days, \n",
    "                                               start_time=start_time)\n",
    "        pool_worker = Pool(processes=8, initializer=init_pool)\n",
    "        pool_worker.map(clean_snowdepth_para_partial, all_sites_id)\n",
    "        pool_worker.close()\n",
    "        pool_worker.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def query_clean_snowdepth(valid_motes, week_start_time):\n",
    "    week_end_time = week_start_time + timedelta(days=7)\n",
    "    site_id_unique = np.unique(valid_motes[:, 0])\n",
    "    site_sd_info = np.empty((0, 4))\n",
    "    for site_id in site_id_unique:\n",
    "        site_valid_motes = valid_motes[valid_motes[:, 0] == site_id]\n",
    "        site_clean_sd = np.empty((1, 0))\n",
    "        elevation_mean = np.nanmean(site_valid_motes[:, 2])\n",
    "        for mote_info in site_valid_motes:\n",
    "            site_id = int(mote_info[0])\n",
    "            node_id = int(mote_info[1])\n",
    "            elevation = int(mote_info[2])\n",
    "            ground_dist = int(mote_info[3])\n",
    "            query_string = \"SELECT sd_clean FROM level_1 WHERE site_id = \" + str(site_id) + \" AND node_id = \" + str(node_id) + \\\n",
    "                           \" AND datetime <= '\" + week_end_time.strftime(\"%Y-%m-%d %H:%M:%S\") + \"' AND datetime >= '\" + \\\n",
    "                           week_start_time.strftime(\"%Y-%m-%d %H:%M:%S\") + \"'\"\n",
    "            clean_sd = pd.read_sql_query(query_string, cnx).as_matrix()\n",
    "            if clean_sd[0, 0] is None:\n",
    "                continue\n",
    "            clean_sd = ground_dist - clean_sd\n",
    "            site_clean_sd = np.append(site_clean_sd, clean_sd)\n",
    "        sd_mean = np.nanmean(site_clean_sd)\n",
    "        sd_std = np.nanstd(site_clean_sd)\n",
    "        site_sd_info = np.vstack((site_sd_info, np.array([site_id, elevation_mean, sd_mean, sd_std])))\n",
    "    return site_sd_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def datetime_to_string(datetime_datetime):\n",
    "    return \"'\" + datetime_datetime.strftime(\"%Y-%m-%d %H:%M:%S\") + \"'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weekly_avg_std_by_site(site_table, clean=False):\n",
    "    week_start_time = datetime.now() - timedelta(days=7)\n",
    "    year = week_start_time.year\n",
    "    month = week_start_time.month\n",
    "    day = week_start_time.day\n",
    "    week_start_time = datetime(year, month, day)\n",
    "    valid_motes = node_info_query_by_site(site_table, week_start_time)\n",
    "    if clean:\n",
    "        clean_snowdepth(site_table, week_start_time)\n",
    "    site_sd_info = query_clean_snowdepth(valid_motes, week_start_time)\n",
    "    site_sd_info = site_sd_info[~np.isnan(site_sd_info[:, 2])]\n",
    "    site_sd_info = site_sd_info[np.argsort(site_sd_info[:, 1])]\n",
    "    site_name = []\n",
    "    for temp_site_sd_info in site_sd_info:\n",
    "        site_name.append(site_table.loc[site_table['site_id'] == int(temp_site_sd_info[0]), 'site_name'].as_matrix()[0])\n",
    "    x = np.array(range(1, len(site_sd_info)+1))\n",
    "    sd = site_sd_info[:, 2] / 10.\n",
    "    sd_std = site_sd_info[:, 3] / 10.\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.xticks(x, site_name)\n",
    "    plt.errorbar(x, sd, yerr=sd_std, fmt='o')\n",
    "    plt.xlim([0, len(site_name)+1])\n",
    "    plt.xlabel(\"Site name, sorted by elevation\")\n",
    "    plt.ylabel(\"Snow depth, cm\")\n",
    "    plt.grid()\n",
    "    plt.savefig(\"week_example.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ts_query_by_site(site_motes, ts_start_time_str, ts_stop_time_str):\n",
    "    sd_matrix = None\n",
    "    datetime_matrix = None\n",
    "    for mote in site_motes:\n",
    "        site_id = int(mote[0])\n",
    "        node_id = int(mote[1])\n",
    "        elevation = int(mote[2])\n",
    "        ground_dist = int(mote[3])\n",
    "        query_str = \"SELECT datetime, sd_clean FROM level_1 WHERE site_id = \" + str(site_id) + \" AND node_id = \" + str(node_id) + \\\n",
    "                    \" AND datetime <= \" + ts_stop_time_str + \" AND datetime >= \" + ts_start_time_str + \" ORDER BY datetime\"\n",
    "        clean_sd_datetime = pd.read_sql_query(query_str, cnx)\n",
    "        clean_sd = clean_sd_datetime.loc[:, 'sd_clean'].as_matrix()\n",
    "        datetime_arr = clean_sd_datetime.loc[:, 'datetime'].as_matrix()\n",
    "        if clean_sd[0] is None:\n",
    "            continue\n",
    "        clean_sd = ground_dist - clean_sd\n",
    "        if sd_matrix is None:\n",
    "            sd_matrix = clean_sd\n",
    "            datetime_matrix = datetime_arr\n",
    "        else:\n",
    "            sd_matrix = np.column_stack((sd_matrix, clean_sd))\n",
    "    return sd_matrix, datetime_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ts_wyd_by_site(site_table, clean = False):\n",
    "    now = datetime.now()\n",
    "    current_month = now.month\n",
    "    current_year = now.year\n",
    "    if current_month >= 10:\n",
    "        wy = current_year + 1\n",
    "    else:\n",
    "        wy = current_year\n",
    "    ts_start_time = datetime(wy-1, 10, 1)\n",
    "    ts_stop_time = datetime(now.year, now.month, now.day)\n",
    "    # If cleaning is true, usually this should be False\n",
    "    if clean:\n",
    "        clean_snowdepth(site_table, ts_start_time, ts_stop_time) \n",
    "    ts_start_time_str = datetime_to_string(ts_start_time)\n",
    "    ts_stop_time_str = datetime_to_string(ts_stop_time)\n",
    "    valid_motes = node_info_query_by_site(site_table, ts_start_time)\n",
    "    valid_motes = valid_motes[valid_motes[:, 3]!=9999.]\n",
    "    unique_sites_id = np.unique(valid_motes[:, 0]).astype(int)\n",
    "    fig, axarr = plt.subplots(len(unique_sites_id), sharex=True, figsize=(20, len(unique_sites_id) * 5))\n",
    "    for i, site_id in enumerate(unique_sites_id):\n",
    "        site_motes = valid_motes[valid_motes[:, 0] == site_id]\n",
    "        site_sd_clean, site_datetime = ts_query_by_site(site_motes, ts_start_time_str, ts_stop_time_str)\n",
    "        site_name = site_table.site_name[site_id-1]\n",
    "        if site_sd_clean is None:\n",
    "            continue\n",
    "        ts_sd_avg = np.nanmean(site_sd_clean, axis=1)\n",
    "        ts_sd_avg /= 10.\n",
    "        ts_sd_std = np.nanstd(site_sd_clean, axis=1)\n",
    "        ts_sd_std /= 10.\n",
    "        data_length = len(ts_sd_std)\n",
    "        axarr[i].plot(site_datetime, ts_sd_avg)\n",
    "        axarr[i].fill_between(site_datetime, ts_sd_avg - ts_sd_std, ts_sd_avg + ts_sd_std, facecolor=\"grey\", alpha=0.6)\n",
    "        axarr[i].set_ylim([0, 300])\n",
    "        axarr[i].text(datetime(wy-1, 10, 5), 250, site_name)\n",
    "        axarr[i].grid()\n",
    "    plt.xlabel(\"Time\")\n",
    "    fig.text(0.06, 0.5, 'Snowdepth, cm', ha='center', va='center', rotation='vertical')\n",
    "    plt.savefig(\"week_ts_example.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def week_report():\n",
    "    site_table = init()\n",
    "    weekly_avg_std_by_site(site_table)\n",
    "    ts_wyd_by_site(site_table)\n",
    "    cnx.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "week_report()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
