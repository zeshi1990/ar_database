{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "__author__ = \"zeshi\"\n",
    "\n",
    "import numpy as np\n",
    "from scipy.linalg import svd\n",
    "import scipy\n",
    "import math\n",
    "from mysqldb_level0 import query_data_level0, site_info_check\n",
    "from level0_2_level1 import query_data_level1\n",
    "import mysql.connector\n",
    "from mysql.connector import errorcode\n",
    "from datetime import datetime, date, timedelta\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init():\n",
    "    global cnx\n",
    "    cnx = mysql.connector.connect(user='root', password='root', database='ar_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__This function should be runned every year before the snow season to figure out the baseline of the snow-depth data__\n",
    "```\n",
    "WY: Water year\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def snowdepth_baseline_update(WY):\n",
    "    all_motes_query = (\"SELECT site_id, node_id FROM motes\")\n",
    "    site_name_query = (\"SELECT site_name FROM sites WHERE site_id = %s\")\n",
    "    baseline_update = (\"UPDATE motes SET ground_dist = %s WHERE site_id = %s AND node_id = %s\")\n",
    "    # old_baseline_query = (\"SELECT ground_dist FROM motes WHERE site_id = %s AND node_id = %s\")\n",
    "    min_datetime = datetime(WY-1, 8, 1)\n",
    "    max_datetime = datetime(WY-1, 10, 1)\n",
    "    cnx = mysql.connector.connect(user='root', password='root', database='ar_data') \n",
    "    cursor = cnx.cursor()\n",
    "    try:\n",
    "        cursor.execute(all_motes_query)\n",
    "    except mysql.connector.Error as err:\n",
    "        print(\"Query motes problem\")\n",
    "        print(err)\n",
    "    all_motes = cursor.fetchall()\n",
    "    for mote in all_motes:\n",
    "        mote_ground_distance = query_data_level1(mote[0], mote[1], min_datetime, max_datetime, field='snowdepth')\n",
    "        mote_ground_distance = np.array([i[0] for i in mote_ground_distance])\n",
    "        mote_ground_distance = mote_ground_distance[mote_ground_distance >= 2000.]\n",
    "        ground_distance = np.nanmean(mote_ground_distance)\n",
    "        if mote_ground_distance.size == 0:\n",
    "            ground_distance = None\n",
    "            print(\"site_id:\" + str(mote[0]) + \" node_id:\" + str(mote[1]) + \" no record for WY:\" + str(WY))\n",
    "        if ground_distance is not None:\n",
    "            try:\n",
    "                cursor.execute(baseline_update, (float(ground_distance), mote[0], mote[1]))\n",
    "            except mysql.connector.Error as err:\n",
    "                print(err)\n",
    "            print(\"site_id:\" + str(mote[0]) + \" node_id:\" + str(mote[1]) + \" baseline update complete!\")\n",
    "    cnx.commit()\n",
    "    cursor.close()\n",
    "    cnx.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pd_query(site_name_id, node_id, starting_time, ending_time):\n",
    "    site_id = site_info_check(site_name_id, node_id)\n",
    "    sql_query = \"SELECT * FROM level_1 WHERE site_id = \" + str(site_id) + \" AND node_id = \" + str(node_id) + \\\n",
    "                     \" AND datetime >= '\" + starting_time.strftime(\"%Y-%m-%d %H:%M:%S\") + \"' AND datetime <= '\" + \\\n",
    "                     ending_time.strftime(\"%Y-%m-%d %H:%M:%S\") + \"'\"\n",
    "    try:\n",
    "        pd_table = pd.read_sql_query(sql_query, cnx)\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        print(\"Querying error happens between pandas and mysql when getting level_1 data.\")\n",
    "    return pd_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pd_query_ground_dist(site_name_id, node_id):\n",
    "    site_id = site_info_check(site_name_id, node_id)\n",
    "    sql_query = \"SELECT ground_dist FROM motes WHERE site_id = \" + str(site_id) + \" AND node_id = \" + str(node_id)\n",
    "    try:\n",
    "        ground_dist = pd.read_sql_query(sql_query, cnx)\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        print(\"Queryiing error happens between pandas and mysql when getting ground_dist from TABLE motes.\")\n",
    "    return ground_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function will run by node, however, it is better to group "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def level1_cleaning_node_query(site_name_id, node_id, starting_time, ending_time):\n",
    "    dirty_table = pd_query(site_name_id, node_id, starting_time, ending_time)\n",
    "    ground_dist = pd_query_ground_dist(site_name_id, node_id).as_matrix()[0, 0]\n",
    "    dirty_sd = np.array(dirty_table['snowdepth'].as_matrix(), dtype=np.float)\n",
    "    dirty_temp = np.array(dirty_table['temperature'].as_matrix(), dtype = np.float)\n",
    "    dirty_rh = np.array(dirty_table['relative_humidity'].as_matrix(), dtype = np.float)\n",
    "    return (dirty_sd, dirty_temp, dirty_rh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pca(data, d):\n",
    "    temp_data = np.copy(data)\n",
    "    temp_data = temp_data\n",
    "    U, S, V = svd(temp_data, full_matrices=False)\n",
    "    U = U[:, :d]\n",
    "    U *= S[:d] ** 0.5\n",
    "    return U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pca_clean(data_matrix, d):\n",
    "    temp_data_matrix = np.copy(data_matrix)\n",
    "    for i in range(0, temp_data_matrix.shape[1]):\n",
    "        temp_data_matrix[np.isnan(temp_data_matrix[:, i]), i] = np.nanmean(temp_data_matrix[:, i])\n",
    "    for i in range(0, temp_data_matrix.shape[0]):\n",
    "        temp_data_matrix[i, np.isnan(temp_data_matrix[i, :])] = np.nanmean(temp_data_matrix[i, :])\n",
    "    s_mode_U = np.matrix(pca(temp_data_matrix, d))\n",
    "    t_mode_U = np.matrix(pca(temp_data_matrix.T, d))\n",
    "    reconstruction = s_mode_U * t_mode_U.T\n",
    "    return reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dineof(input_data, n_max = None, max_Iter = 100, rms_inc = 1e-5):\n",
    "    data = np.copy(input_data)\n",
    "    if n_max is None:\n",
    "        n_max = data.shape[1]\n",
    "    is_nan = np.isnan(data)\n",
    "    not_nan = np.isfinite(data)\n",
    "    recon = np.array(pca_clean(data, n_max))\n",
    "    rms_prev = float('Inf')\n",
    "    rms_now = np.sqrt(np.sum(np.square((recon[not_nan] - data[not_nan]))))\n",
    "    data[is_nan] = recon[is_nan]\n",
    "    iteration = 1\n",
    "    while ((rms_prev - rms_now) > rms_inc) and iteration <= max_Iter:\n",
    "        iteration += 1\n",
    "        recon = np.array(pca_clean(data, n_max))\n",
    "        rms_temp = np.sqrt(np.sum(np.square((recon[not_nan] - data[not_nan]))))\n",
    "        if rms_temp < rms_now:\n",
    "            rms_prev = np.copy(rms_now)\n",
    "            rms_now = np.copy(rms_temp)\n",
    "        data[is_nan] = recon[is_nan]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CollaborativeFiltering:\n",
    "    def __init__(self, R, r, tol = 1e-2, maxIter = 1000, l = 1e-3, mu = 1e-3):\n",
    "        self.R = R\n",
    "        self.r = r\n",
    "        self.tol = tol\n",
    "        self.maxIter = maxIter\n",
    "        self.l = l\n",
    "        self.mu = mu\n",
    "        self.numUser = R.shape[0]\n",
    "        self.numItem = R.shape[1]\n",
    "        ## u should be number of users * r\n",
    "        self.u = np.random.randn(R.shape[0], r) * math.sqrt(10)\n",
    "        ## u should be number of r * number of itemss\n",
    "        self.v = np.random.randn(r, R.shape[1]) * math.sqrt(10)\n",
    "        self.Iter = 0\n",
    "        self.loss = float('inf')\n",
    "        \n",
    "    def train(self):\n",
    "        new_u = self.updateU(self.v)\n",
    "        new_v = self.updateV(new_u)\n",
    "        i = 1.\n",
    "        while (np.linalg.norm(self.u - new_u) > self.tol or \n",
    "               np.linalg.norm(self.v - new_v) > self.tol) and self.Iter < self.maxIter:\n",
    "            self.l = 0.01/i\n",
    "            self.mu = 0.01/i\n",
    "            i += 1.\n",
    "            self.Iter += 1\n",
    "            self.u = new_u\n",
    "            self.v = new_v\n",
    "            new_u = self.updateU(new_v)\n",
    "            new_v = self.updateV(new_u)\n",
    "            \n",
    "    def calculateLoss(self):\n",
    "        self.loss = np.linalg.norm(np.nan_to_num(np.dot(self.u, self.v) - self.R)) + \\\n",
    "                    self.l * np.linalg.norm(self.u) + \\\n",
    "                    self.mu * np.linalg.norm(self.v)\n",
    "    \n",
    "    def updateU(self, v):\n",
    "        new_u = np.zeros((self.numUser, self.r))\n",
    "        for i in range(self.numUser):\n",
    "            ## get the index of non nan v index:\n",
    "            v_indexes = np.isfinite(self.R[i, :])\n",
    "            v_v_indexes = v[:, v_indexes]\n",
    "            R_v_indexes = self.R[i, v_indexes]\n",
    "            ## update u[i]\n",
    "            u_i = np.linalg.solve(np.dot(v_v_indexes, v_v_indexes.T) + self.l * np.eye(self.r), \n",
    "                                  np.dot(v_v_indexes, R_v_indexes.T)).T\n",
    "            new_u[i] = u_i\n",
    "        return new_u\n",
    "    \n",
    "    def updateV(self, u):\n",
    "        new_v = np.zeros((self.r, self.numItem))\n",
    "        for i in range(self.numItem):\n",
    "            ## get the index of non nan v index:\n",
    "            u_indexes = np.isfinite(self.R[:, i])\n",
    "            u_u_indexes = u[u_indexes, :]\n",
    "            R_u_indexes = self.R[u_indexes, i]\n",
    "            ## update u[i]\n",
    "            v_i = np.linalg.solve(np.dot(u_u_indexes.T, u_u_indexes) + self.mu * np.eye(self.r), \n",
    "                                  np.dot(u_u_indexes.T, R_u_indexes))\n",
    "            new_v[:,i] = v_i\n",
    "        return new_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def level1_cleaning_site_pca_clean(site_name_id, site_num_of_nodes, starting_time, ending_time):\n",
    "    retained_list = []\n",
    "    sd_matrix = None\n",
    "    temp_matrix = None\n",
    "    rh_matrix = None\n",
    "    for temp_node_id in range(1, site_num_of_nodes + 1):\n",
    "        (temp_dirty_sd, temp_dirty_temp, temp_dirty_rh) = level1_cleaning_node_query(site_name_id, \n",
    "                                                                                     temp_node_id, \n",
    "                                                                                     starting_time, \n",
    "                                                                                     ending_time)\n",
    "        temp_data_length = len(temp_dirty_sd)\n",
    "        # Clean temperature, rh. And clean the snow depth for the first round\n",
    "        temp_dirty_temp[np.logical_or(temp_dirty_temp >= 100., temp_dirty_temp <= -30.)] = np.nan\n",
    "        temp_dirty_rh[np.logical_or(temp_dirty_rh >= 100., temp_dirty_rh <= 0.)] = np.nan\n",
    "        temp_dirty_sd[np.logical_or(temp_dirty_sd >= 5000., temp_dirty_sd <= 600.)] = np.nan\n",
    "        last_not_nan = None\n",
    "        last_not_nan_idx = None\n",
    "        # Clean snow depth data one by one\n",
    "        for i in range(0, temp_data_length):\n",
    "            if not np.isnan(temp_dirty_sd[i]):\n",
    "                if last_not_nan is not None:\n",
    "                    if abs(temp_dirty_sd[i] - last_not_nan) >= 100 and (i - last_not_nan_idx) <= 10:\n",
    "                        temp_dirty_sd[i] = np.nan\n",
    "                    if abs(temp_dirty_sd[i] - last_not_nan) >= 400 and (i - last_not_nan_idx) <= 100:\n",
    "                        temp_dirty_sd[i] = np.nan\n",
    "                else:\n",
    "                    last_not_nan = temp_dirty_sd[i]\n",
    "                    last_not_nan_idx = i\n",
    "        temp_data_not_nan = len(np.where(~np.isnan(temp_dirty_sd))[0])\n",
    "        if temp_data_not_nan < 0.5 * float(temp_data_length) or temp_data_length == 0:\n",
    "            continue\n",
    "        retained_list.append(temp_node_id)\n",
    "        if sd_matrix is None:\n",
    "            sd_matrix = temp_dirty_sd\n",
    "            temp_matrix = temp_dirty_temp\n",
    "            rh_matrix = temp_dirty_rh\n",
    "        else:\n",
    "            sd_matrix = np.column_stack((sd_matrix, temp_dirty_sd))\n",
    "            temp_matrix = np.column_stack((temp_matrix, temp_dirty_temp))\n",
    "            rh_matrix = np.column_stack((rh_matrix, temp_dirty_rh))\n",
    "    if len(retained_list) == 0 or len(retained_list) == 1:\n",
    "        return (None, None, None, None)\n",
    "    if sd_matrix.shape[1] >= 5:\n",
    "        num_pc = sd_matrix.shape[1]\n",
    "    else:\n",
    "        num_pc = 2\n",
    "    original_sd = np.copy(sd_matrix)\n",
    "    sd_recon = dineof(sd_matrix, n_max=num_pc)\n",
    "    sd_matrix[np.isnan(sd_matrix)] = sd_recon[np.isnan(sd_matrix)]\n",
    "    temp_recon = dineof(temp_matrix, n_max=num_pc)\n",
    "    temp_matrix[np.isnan(temp_matrix)] = temp_recon[np.isnan(temp_matrix)]\n",
    "    rh_recon = dineof(rh_matrix, n_max=num_pc)\n",
    "    rh_matrix[np.isnan(rh_matrix)] = rh_recon[np.isnan(rh_matrix)]\n",
    "    return (retained_list, sd_matrix, temp_matrix, rh_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def level1_cleaning_site_clean_update(site_id, retained_list, datetime_list, sd_clean, temp_clean, rh_clean):\n",
    "    update_string = (\"UPDATE level_1 SET sd_clean = %s, tmp_clean = %s, rh_clean = %s WHERE site_id = %s \" \n",
    "                     \"AND node_id = %s AND datetime = %s\")\n",
    "    update_data = ()\n",
    "    for i, node_id in enumerate(retained_list):\n",
    "        for j, temp_datetime in enumerate(datetime_list):\n",
    "            update_data += ((float(sd_clean[j, i]), float(temp_clean[j, i]), float(rh_clean[j, i]), \n",
    "                             int(site_id), int(node_id), temp_datetime),)\n",
    "    cursor = cnx.cursor()\n",
    "    try:\n",
    "        cursor.executemany(update_string, update_data)\n",
    "        cnx.commit()\n",
    "    except mysql.connector.Error as err:\n",
    "        print(err)\n",
    "        print(\"Problems when updating cleaned data\")\n",
    "    cursor.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning should be done daily!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def level1_cleaning_site(site_name_id, starting_time, ending_time):\n",
    "    init()\n",
    "    if isinstance(site_name_id, str):\n",
    "        query_string = (\"SELECT site_id, num_of_nodes FROM sites WHERE site_name = '\" + site_name_id + \"'\")\n",
    "        \n",
    "    else:\n",
    "        query_string = (\"SELECT site_id, num_of_nodes FROM sites WHERE site_id = \" + str(site_name_id))\n",
    "    try:\n",
    "        site_info = pd.read_sql_query(query_string, cnx)\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        print(\"Querying site info error\")\n",
    "    site_id = site_info[\"site_id\"].as_matrix()[0]\n",
    "    site_num_of_nodes = site_info[\"num_of_nodes\"].as_matrix()[0]\n",
    "    retained_list, sd_clean, temp_clean, rh_clean = level1_cleaning_site_pca_clean(site_name_id, \n",
    "                                                                                   site_num_of_nodes,\n",
    "                                                                                   starting_time,\n",
    "                                                                                   ending_time)\n",
    "    if retained_list is not None:\n",
    "        datetime_list = []\n",
    "        temp_datetime = starting_time\n",
    "        while temp_datetime <= ending_time:\n",
    "            datetime_list.append(temp_datetime)\n",
    "            temp_datetime += timedelta(minutes=15)\n",
    "        level1_cleaning_site_clean_update(site_id, retained_list, datetime_list, sd_clean, temp_clean, rh_clean)\n",
    "    cnx.close()\n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
