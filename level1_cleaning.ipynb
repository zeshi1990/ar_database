{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "__author__ = \"zeshi\"\n",
    "\n",
    "import numpy as np\n",
    "from scipy.linalg import svd\n",
    "from mysqldb_level_0 import query_data_level0, site_info_check\n",
    "from level0_2_level1 import query_data_level1\n",
    "import mysql.connector\n",
    "from mysql.connector import errorcode\n",
    "from datetime import datetime, date, timedelta\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__This function should be runned every year before the snow season to figure out the baseline of the snow-depth data__\n",
    "```\n",
    "WY: Water year\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def snowdepth_baseline_update(WY):\n",
    "    all_motes_query = (\"SELECT site_id, node_id FROM motes\")\n",
    "    site_name_query = (\"SELECT site_name FROM sites WHERE site_id = %s\")\n",
    "    baseline_update = (\"UPDATE motes SET ground_dist = %s WHERE site_id = %s AND node_id = %s\")\n",
    "    # old_baseline_query = (\"SELECT ground_dist FROM motes WHERE site_id = %s AND node_id = %s\")\n",
    "    min_datetime = datetime(WY-1, 8, 1)\n",
    "    max_datetime = datetime(WY-1, 10, 1)\n",
    "    cnx = mysql.connector.connect(user='root', password='root', database='ar_data') \n",
    "    cursor = cnx.cursor()\n",
    "    try:\n",
    "        cursor.execute(all_motes_query)\n",
    "    except mysql.connector.Error as err:\n",
    "        print(\"Query motes problem\")\n",
    "        print(err)\n",
    "    all_motes = cursor.fetchall()\n",
    "    for mote in all_motes:\n",
    "        mote_ground_distance = query_data_level1(mote[0], mote[1], min_datetime, max_datetime, field='snowdepth')\n",
    "        mote_ground_distance = np.array([i[0] for i in mote_ground_distance])\n",
    "        mote_ground_distance = mote_ground_distance[mote_ground_distance >= 2000.]\n",
    "        ground_distance = np.nanmean(mote_ground_distance)\n",
    "        if mote_ground_distance.size == 0:\n",
    "            ground_distance = None\n",
    "            print(\"site_id:\" + str(mote[0]) + \" node_id:\" + str(mote[1]) + \" no record for WY:\" + str(WY))\n",
    "        if ground_distance is not None:\n",
    "            try:\n",
    "                cursor.execute(baseline_update, (float(ground_distance), mote[0], mote[1]))\n",
    "            except mysql.connector.Error as err:\n",
    "                print(err)\n",
    "            print(\"site_id:\" + str(mote[0]) + \" node_id:\" + str(mote[1]) + \" baseline update complete!\")\n",
    "    cnx.commit()\n",
    "    cursor.close()\n",
    "    cnx.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pd_query(site_name_id, node_id, starting_time, ending_time):\n",
    "    site_id = site_info_check(site_name_id, node_id)\n",
    "    sql_query = \"SELECT * FROM level_1 WHERE site_id = \" + str(site_id) + \" AND node_id = \" + str(node_id) + \\\n",
    "                     \" AND datetime >= '\" + starting_time.strftime(\"%Y-%m-%d %H:%M:%S\") + \"' AND datetime <= '\" + \\\n",
    "                     ending_time.strftime(\"%Y-%m-%d %H:%M:%S\") + \"'\"\n",
    "    cnx = mysql.connector.connect(user = \"root\", password = \"root\", database = \"ar_data\")\n",
    "    try:\n",
    "        pd_table = pd.read_sql_query(sql_query, cnx)\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        print(\"Querying error happens between pandas and mysql when getting level_1 data.\")\n",
    "    cnx.close()\n",
    "    return pd_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pd_query_ground_dist(site_name_id, node_id):\n",
    "    site_id = site_info_check(site_name_id, node_id)\n",
    "    sql_query = \"SELECT ground_dist FROM motes WHERE site_id = \" + str(site_id) + \" AND node_id = \" + str(node_id)\n",
    "    cnx = mysql.connector.connect(user = \"root\", password = \"root\", database = \"ar_data\")\n",
    "    try:\n",
    "        ground_dist = pd.read_sql_query(sql_query, cnx)\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        print(\"Queryiing error happens between pandas and mysql when getting ground_dist from TABLE motes.\")\n",
    "    cnx.close()\n",
    "    return ground_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function will run by node, however, it is better to group "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def level1_cleaning_node_query(site_name_id, node_id, starting_time, ending_time):\n",
    "    dirty_table = pd_query(site_name_id, node_id, starting_time, ending_time)\n",
    "    ground_dist = pd_query_ground_dist(site_name_id, node_id).as_matrix()[0, 0]\n",
    "    dirty_sd = np.array(dirty_table['snowdepth'].as_matrix(), dtype=np.float)\n",
    "    dirty_temp = np.array(dirty_table['temperature'].as_matrix(), dtype = np.float)\n",
    "    dirty_rh = np.array(dirty_table['relative_humidity'].as_matrix(), dtype = np.float)\n",
    "    return (dirty_sd, dirty_temp, dirty_rh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pca(data, d):\n",
    "    temp_data = np.copy(data)\n",
    "    temp_data = temp_data\n",
    "    U, S, V = svd(temp_data, full_matrices=False)\n",
    "    U = U[:, :d]\n",
    "    U *= S[:d] ** 0.5\n",
    "    return U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pca_clean(data_matrix, d):\n",
    "    temp_data_matrix = np.copy(data_matrix)\n",
    "    for i in range(0, temp_data_matrix.shape[1]):\n",
    "        temp_data_matrix[np.isnan(temp_data_matrix[:, i]), i] = np.nanmean(temp_data_matrix[:, i])\n",
    "    s_mode_U = np.matrix(pca(temp_data_matrix, d))\n",
    "    t_mode_U = np.matrix(pca(temp_data_matrix.T, d))\n",
    "    reconstruction = s_mode_U * t_mode_U.T\n",
    "    return reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def level1_cleaning_site_pca_clean(site_name_id, site_num_of_nodes, starting_time, ending_time):\n",
    "    retained_list = []\n",
    "    sd_matrix = None\n",
    "    temp_matrix = None\n",
    "    rh_matrix = None\n",
    "    for temp_node_id in range(1, site_num_of_nodes + 1):\n",
    "        (temp_dirty_sd, temp_dirty_temp, temp_dirty_rh) = level1_cleaning_node_query(site_name_id, \n",
    "                                                                                     temp_node_id, \n",
    "                                                                                     starting_time, \n",
    "                                                                                     ending_time)\n",
    "        temp_data_length = len(temp_dirty_sd)\n",
    "        # Clean temperature, rh. And clean the snow depth for the first round\n",
    "        temp_dirty_temp[np.logical_or(temp_dirty_temp >= 100., temp_dirty_temp <= -30.)] = np.nan\n",
    "        temp_dirty_rh[np.logical_or(temp_dirty_rh >= 100., temp_dirty_rh <= 0.)] = np.nan\n",
    "        temp_dirty_sd[np.logical_or(temp_dirty_sd >= 5000., temp_dirty_sd <= 600.)] = np.nan\n",
    "        last_not_nan = None\n",
    "        last_not_nan_idx = None\n",
    "        # Clean snow depth data one by one\n",
    "        for i in range(0, temp_data_length):\n",
    "            if not np.isnan(temp_dirty_sd[i]):\n",
    "                if last_not_nan is not None:\n",
    "                    if abs(temp_dirty_sd[i] - last_not_nan) >= 100 and (i - last_not_nan_idx) <= 10:\n",
    "                        temp_dirty_sd[i] = np.nan\n",
    "                    if abs(temp_dirty_sd[i] - last_not_nan) >= 400 and (i - last_not_nan_idx) <= 100:\n",
    "                        temp_dirty_sd[i] = np.nan\n",
    "                else:\n",
    "                    last_not_nan = temp_dirty_sd[i]\n",
    "                    last_not_nan_idx = i\n",
    "        temp_data_not_nan = len(np.where(~np.isnan(temp_dirty_sd))[0])\n",
    "        if temp_data_not_nan < 0.5 * float(temp_data_length) or temp_data_length == 0:\n",
    "            continue\n",
    "        retained_list.append(temp_node_id)\n",
    "        if sd_matrix is None:\n",
    "            sd_matrix = temp_dirty_sd\n",
    "            temp_matrix = temp_dirty_temp\n",
    "            rh_matrix = temp_dirty_rh\n",
    "        else:\n",
    "            sd_matrix = np.column_stack((sd_matrix, temp_dirty_sd))\n",
    "            temp_matrix = np.column_stack((temp_matrix, temp_dirty_temp))\n",
    "            rh_matrix = np.column_stack((rh_matrix, temp_dirty_rh))\n",
    "    if len(retained_list) == 0 or len(retained_list) == 1:\n",
    "        return (None, None, None, None)\n",
    "    if sd_matrix.shape[1] >= 5:\n",
    "        num_pc = sd_matrix.shape[1]\n",
    "    else:\n",
    "        num_pc = 2\n",
    "    original_sd = np.copy(sd_matrix)\n",
    "    sd_recon = np.array(pca_clean(sd_matrix, num_pc))\n",
    "    sd_matrix[np.isnan(sd_matrix)] = sd_recon[np.isnan(sd_matrix)]\n",
    "    temp_recon = np.array(pca_clean(temp_matrix, num_pc))\n",
    "    temp_matrix[np.isnan(temp_matrix)] = temp_recon[np.isnan(temp_matrix)]\n",
    "    rh_recon = np.array(pca_clean(rh_matrix, num_pc))\n",
    "    rh_matrix[np.isnan(rh_matrix)] = rh_recon[np.isnan(rh_matrix)]\n",
    "#     return (retained_list, sd_matrix, temp_matrix, rh_matrix)\n",
    "    return(sd_matrix, original_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def level1_cleaning_site_clean_update(site_id, retained_list, datetime_list, sd_clean, temp_clean, rh_clean):\n",
    "    update_string = (\"UPDATE level_1 SET sd_clean = %s, tmp_clean = %s, rh_clean = %s WHERE site_id = %s \" \n",
    "                     \"AND node_id = %s AND datetime = %s\")\n",
    "    update_data = ()\n",
    "    for i, node_id in enumerate(retained_list):\n",
    "        for j, temp_datetime in enumerate(datetime_list):\n",
    "            update_data += ((float(sd_clean[j, i]), float(temp_clean[j, i]), float(rh_clean[j, i]), \n",
    "                             int(site_id), int(node_id), temp_datetime),)\n",
    "    cnx = mysql.connector.connect(user = \"root\", password = \"root\", database = \"ar_data\")\n",
    "    cursor = cnx.cursor()\n",
    "    try:\n",
    "        cursor.executemany(update_string, update_data)\n",
    "        cnx.commit()\n",
    "    except mysql.connector.Error as err:\n",
    "        print(err)\n",
    "        print(\"Problems when updating cleaned data\")\n",
    "    cursor.close()\n",
    "    cnx.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning should be done daily!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def level1_cleaning_site(site_name_id, starting_time, ending_time):\n",
    "    if isinstance(site_name_id, str):\n",
    "        query_string = (\"SELECT site_id, num_of_nodes FROM sites WHERE site_name = '\" + site_name_id + \"'\")\n",
    "        \n",
    "    else:\n",
    "        query_string = (\"SELECT site_id, num_of_nodes FROM sites WHERE site_id = \" + str(site_name_id))\n",
    "    cnx = mysql.connector.connect(user = \"root\", password = \"root\", database = \"ar_data\")\n",
    "    try:\n",
    "        site_info = pd.read_sql_query(query_string, cnx)\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        print(\"Querying site info error\")\n",
    "    cnx.close()\n",
    "    site_id = site_info[\"site_id\"].as_matrix()[0]\n",
    "    site_num_of_nodes = site_info[\"num_of_nodes\"].as_matrix()[0]\n",
    "    retained_list, sd_clean, temp_clean, rh_clean = level1_cleaning_site_pca_clean(site_name_id, \n",
    "                                                                                   site_num_of_nodes,\n",
    "                                                                                   starting_time,\n",
    "                                                                                   ending_time)\n",
    "    if retained_list is not None:\n",
    "        datetime_list = []\n",
    "        temp_datetime = starting_time\n",
    "        while temp_datetime <= ending_time:\n",
    "            datetime_list.append(temp_datetime)\n",
    "            temp_datetime += timedelta(minutes=15)\n",
    "        level1_cleaning_site_clean_update(site_id, retained_list, datetime_list, sd_clean, temp_clean, rh_clean)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sd, temp, rh = level1_cleaning_node_query(\"Duncan_Pk\", 6, datetime(2016, 1, 25), datetime(2016, 2, 1))\n",
    "# print (np.where(~np.isnan(sd)))\n",
    "# print(len(np.where(~np.isnan(sd))[0]))\n",
    "# a = np.array([1,2,3,np.nan,4,5,101])\n",
    "# print(a)\n",
    "# a[a>100] = np.nan\n",
    "# print(a)\n",
    "# print(np.isnan(np.nan))\n",
    "# level1_cleaning_site_pca_clean(\"Duncan_Pk\", 11, datetime(2016, 1, 25), datetime(2016, 2, 1))\n",
    "# a, b = level1_cleaning_site_pca_clean(\"Duncan_Pk\", 11, datetime(2016, 1, 25), datetime(2016, 1, 26))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
