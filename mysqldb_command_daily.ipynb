{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "__author__ = \"zeshi\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import mysql.connector\n",
    "from mysql.connector import errorcode\n",
    "from mysqldb_level0 import populate_data_server\n",
    "from level0_2_level1 import level0_to_level1_data_merge\n",
    "\n",
    "# rsync data from webserver and transfer data from local to local\n",
    "os.system(\"python /media/raid0/zeshi/AR_db/rsync_ssh.py\")\n",
    "os.system(\"python /media/raid0/zeshi/AR_db/tmp_to_server_data.py\")\n",
    "\n",
    "print(\"Finished transfer data from webserver to compserver!\")\n",
    "\n",
    "# Query site_names from mysql and populate server data into level_0 table\n",
    "cnx = mysql.connector.connect(user = \"root\", password = \"root\", database = \"ar_data\")\n",
    "cursor = cnx.cursor()\n",
    "try:\n",
    "    cursor.execute(\"SELECT site_name FROM sites\")\n",
    "    site_names = cursor.fetchall()\n",
    "except mysql.connector.Error as err:\n",
    "    print(err)\n",
    "cursor.close()\n",
    "cnx.close()\n",
    "\n",
    "site_names_list = []\n",
    "\n",
    "for site_name in site_names:\n",
    "    site_names_list.append(site_name[0])\n",
    "\n",
    "# Initialize parallel processing\n",
    "pool = Pool(processes=cpu_count())\n",
    "# Mapping populate data server with the site_names_list\n",
    "pool.map(populate_data_server, site_names_list)\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "# Query site_name and number of node at each site\n",
    "cnx = mysql.connector.connect(user = \"root\", password = \"root\", database = \"ar_data\")\n",
    "cursor = cnx.cursor()\n",
    "try:\n",
    "    cursor.execute(\"SELECT site_name, num_of_nodes FROM sites\")\n",
    "    sites_infos = cursor.fetchall()\n",
    "except mysql.connector.Error as err:\n",
    "    print(err)\n",
    "cursor.close()\n",
    "cnx.close()\n",
    "\n",
    "def merge0_to_1_parallel(site_info):\n",
    "    starting_time = datetime.today().replace(hour=0, minute=0, second=0, microsecond=0) - timedelta(days=1)\n",
    "    ending_time = datetime.today().replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    site_name = site_info[0]\n",
    "    site_num_of_nodes = site_info[1]\n",
    "    for node_id in range(1, site_num_of_nodes + 1):\n",
    "        level0_to_level1_data_merge(site_name, node_id, datetime_range_interupt=(starting_time, ending_time))\n",
    "    \n",
    "# Initizalize parallel processing\n",
    "pool = Pool(processes=cpu_count())\n",
    "# Merge level_0 data to level_1 data\n",
    "pool.map(merge0_to_1_parallel, sites_infos)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def long_term_update(starting_time, real_ending_time):\n",
    "    # Query site_infos, site_name and number of nodes\n",
    "    cnx = mysql.connector.connect(user = \"root\", password = \"root\", database = \"ar_data\")\n",
    "    cursor = cnx.cursor()\n",
    "    try:\n",
    "        cursor.execute(\"SELECT site_name, num_of_nodes FROM sites\")\n",
    "        sites_infos = cursor.fetchall()\n",
    "    except mysql.connector.Error as err:\n",
    "        print(err)\n",
    "    cursor.close()\n",
    "    cnx.close()\n",
    "    \n",
    "    # ending_time is 7 days later than starting date\n",
    "    ending_time = starting_time + timedelta(days=7)\n",
    "    while ending_time <= real_ending_time:\n",
    "        print(starting_time, ending_time)\n",
    "        for site_info in sites_infos:\n",
    "            site_name = site_info[0]\n",
    "            site_num_of_nodes = site_info[1]\n",
    "            for node_id in range(1, site_num_of_nodes + 1):\n",
    "                level0_to_level1_data_merge(site_name, node_id, datetime_range_interupt=(starting_time, ending_time))\n",
    "        starting_time += timedelta(days=7)\n",
    "        ending_time += timedelta(days=7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
